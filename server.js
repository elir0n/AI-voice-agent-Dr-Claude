import express from "express";
import dotenv from "dotenv";
import OpenAI from "openai";
import multer from "multer";
import cors from "cors";
import fs from "fs";
import ffmpeg from "fluent-ffmpeg";
import ffmpegPath from "ffmpeg-static";
import { generateAvailableAppointments } from "./src/services/appointmentService.js";
import { saveLog, getLogs } from "./src/services/logService.js";
import { agentPrompt } from "./src/rules/agentRules.js";
import { detectLanguage } from "./src/utils/languageDetection.js";
import { textToSpeech } from "./src/services/ttsService.js";

dotenv.config();
const app = express();
app.use(express.json());
app.use(cors());
app.use(express.static("public"));

ffmpeg.setFfmpegPath(ffmpegPath);
const upload = multer({ dest: "uploads/" });
const client = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });
const PORT = process.env.PORT || 3000;

// ðŸ§  ×–×™×›×¨×•×Ÿ ×–×ž× ×™ ×œ×©×™×—×•×ª ×¤×¢×™×œ×•×ª
const sessions = new Map();

if (!fs.existsSync("responses")) fs.mkdirSync("responses");

app.get("/appointments", (req, res) => {
  const data = generateAvailableAppointments();
  res.json(data);
});

app.get("/logs", (req, res) => {
  const rows = getLogs();
  res.json(rows);
});

app.post("/voice", upload.single("audio"), async (req, res) => {
  const inputPath = req.file.path;
  const outputPath = `${inputPath}.mp3`;
  const sessionId = req.body.sessionId || "anonymous";

  try {
    // ðŸŽ§ Convert to mp3
    await new Promise((resolve, reject) => {
      ffmpeg(inputPath)
        .toFormat("mp3")
        .on("end", resolve)
        .on("error", reject)
        .save(outputPath);
    });

    // ðŸŽ¤ Speech â†’ Text
    const transcription = await client.audio.transcriptions.create({
      file: fs.createReadStream(outputPath),
      model: "whisper-1"
    });

    const text = transcription.text.trim();
    console.log("ðŸ—£ï¸ ×“×™×‘×•×¨ â†’ ×˜×§×¡×˜:", text);

    // ðŸ§  Detect language
    const detectedLang = await detectLanguage(text);

    // âœ… × ×™×”×•×œ ×©×™×—×” ×œ×¤×™ session
    if (!sessions.has(sessionId)) {
      sessions.set(sessionId, [
        { role: "system", content: agentPrompt },
        { role: "assistant", content: "×©×œ×•×, ×× ×™ ×”×¢×•×–×¨ ×”×§×•×œ×™ ×©×œ ×“\"×¨ ×§×œ×•×“ ×¤×™×§××¨. ××™×š ××¤×©×¨ ×œ×¢×–×•×¨?" }
      ]);
    }

    // ×ž×•×¡×™×¤×™× ××ª ×”×•×“×¢×ª ×”×ž×©×ª×ž×© ×œ×©×™×—×”
    const conversation = sessions.get(sessionId);
    conversation.push({ role: "user", content: text });

    // ×©×•×œ×—×™× ×œ-GPT ××ª ×›×œ ×”×”×™×¡×˜×•×¨×™×”
    const completion = await client.chat.completions.create({
      model: "gpt-4o",
      messages: conversation,
    });

    const reply = completion.choices[0].message.content.trim();
    console.log("ðŸ¤– GPT:", reply);

    // ×ž×•×¡×™×¤×™× ××ª ×ª×’×•×‘×ª ×”×¡×•×›×Ÿ ×œ×”×™×¡×˜×•×¨×™×”
    conversation.push({ role: "assistant", content: reply });

    // ðŸ’¾ Save log
    saveLog(detectedLang, text, reply);

    // ðŸ”Š Text â†’ Speech
    const audioBuffer = await textToSpeech(reply, outputPath);

    // ðŸ•’ Save response locally
    const timestamp = new Date().toISOString().replace(/[:.]/g, "-");
    const responseAudioPath = `responses/response_${timestamp}.mp3`;
    const responseTextPath = `responses/response_${timestamp}.txt`;

    fs.writeFileSync(responseAudioPath, audioBuffer);
    fs.writeFileSync(
      responseTextPath,
      `ðŸ•“ ${new Date().toLocaleString("he-IL")}\nðŸŒ Language: ${detectedLang}\nðŸŽ¤ User: ${text}\nðŸ¤– GPT: ${reply}\n-------------------------------------\n`
    );

    res.setHeader("Content-Type", "audio/mpeg");
    res.send(audioBuffer);

  } catch (error) {
    console.error(error);
    res.status(500).send("Error processing voice request");
  } finally {
    if (fs.existsSync(inputPath)) fs.unlinkSync(inputPath);
    if (fs.existsSync(outputPath)) fs.unlinkSync(outputPath);
  }
});

app.listen(PORT, () => console.log(`Server running on http://localhost:${PORT}`));
